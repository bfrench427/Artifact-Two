# Artifact Two Narrative

## Briefly describe the artifact. What is it? When was it created?
The artifact chosen for the second enhancement originates from the CS-370: Current/Emerging Trends in Computer Science course. The artifact itself is a reinforcement learning algorithm based on deep q-learning that teaches an intelligent agent to seek the ending to a maze within a game. The program utilized two different Python files that established the rules, rewards, and structure of the maze for the game. Then a Python Notebook file with the code necessary to run training operations so that the agent could learn the game from scratch until it can win with 100% certainty. 
## Justify the inclusion of the artifact in your ePortfolio. Why did you select this item? What specific components of the artifact showcase your skills and abilities in software development? How did the enhancement improve the artifact? What specific skills did you demonstrate in the enhancement?
I selected this artifact for enhancement because I was not satisfied with the original results from the reinforcement learning algorithm. The algorithm did achieve success as the agent was able to achieve a 100%-win rate after hundreds of epochs and about 46 minutes of play time. But I am certain that the algorithm could be improved to reduce the amount of training needed for the agent to produce the same results. Hence why the artifact has been selected for enhancement within this category. As for the specific components that showcase my skills, the incorporation of a reinforcement learning algorithm shows the ability to design and implement algorithm solutions. Improving upon the learning rate of the intelligent agent through the reinforcement learning algorithm shows the ability to improve algorithm efficiency, and effective manipulation of algorithm hyperparameters. The current model used for the reinforcement learning algorithm is model-free, showcasing comprehension of algorithm models and how to establish them. Unfortunately, the enhancment for this artifact did not go as planned. The original artifact conducted training within a virtual desktop environment to increase learning rate. I was not able to replicate the same conditions on my own hardware, causing the learning rate to be 1000 times slower on average. In the future, I will need to try and get a virtual desktop service to run on my computer to train the intelligent agent under similar conditions as the original artifact and achieve better results. Eventhough I was not able to complete the enhancement, I did demonstrate problem solving and critical-thinking abilities towards a potential solutions to the problems I have faced throughout enhancement. As I was able to narrow down what is necessary to achieve a similar level of performance to the original artifact submission and can take measures in the future to finish the enhancment as planned.
## Reflect on the process of enhancing the artifact. What did you learn as you were creating it and improving it? What challenges did you face? How did you incorporate feedback as you made changes to the artifact? How was the artifact improved? Which course outcomes did you partially or fully meet with your enhancements? Which do you feel were not met?
I was aware how sensitive a reinforcement learning algorithm can be to the manipulation of hyperparameters, but now I have seen it firsthand. I have conducted tests with many different values for epochs and batch size to compare with the original results. And as hard as it is to believe, there has not even been a slight improvement to the learning rate of the intelligent agent. The experience of enhancing this artifact has really taught me how much effort it can be to improve algorithm efficiency with even something as simple as changing a few variable values. Testing different hyperparameter combinations was a big challenge due to the time required for each test. The original settings of the algorithm achieved 100%-win rate in about 46 minutes, meaning that each test is to be given just as much to show improvement. It is a lengthy process that did not yield any success, but I now know what is required to achieve enhancement. My instructor did give me feedback to complete training in Google Colab and give the subscription a try for increased GPU performance and computing units. This did not improve the performance of the algorithm in any meaningful way though, as it was still much slower than the Apporto virutal desktop used during the CS-370 course. The course outcome I outlined to be covered by this enhancement was the third outcome. This outcome is to design and evaluate computing solutions that solve a given problem using algorithmic principles and computer science practices and standards appropriate to its solution while managing the trade-offs involved in design choices. Given that the original problem of achieving 100%-win rate through the reinforcement learning algorithm has been achieved, I have shown my ability to design solutions through algorithmic principles while managing trade-offs. This is because reinforcement learning algorithms themselves are a balancing act of managing hyperparameters and choosing the correct model for the problem. Each aspect greatly affects the learning speed of an intelligent agent, resulting in much longer training if improperly handled. So I was able to meet this outcome in my original submission, but I could not do so again with my enhancement.
## Running this Program
To run this program, each of the files included in this portfolio must be added to a new root folder of any name. The name of the root folder used throughout the artifact's development was "TreasureGameHunt". Once a new root folder is made and has the files within it, some software will need to be installed to run the program properly.
1. Python 3
2. TensorFlow
3. Keras
4. NumPy
5. Matplotlib
6. IDE that can run Python Notebooks (Jupyter Notebooks, Visual Studio Code, etc.)

Once everything is installed, open the IDE and either upload or open the root folder with all the files in it. Then, select the option to run all code blocks within the "4-2 Milestone Three Enhancment Two.ipynb" file. Training output will appear below the block containing this code:
```
model = build_model(maze)
qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)
```
